{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import get_config\n",
    "import argparse\n",
    "from ShapeLearner import ShapeLearner\n",
    "from ShapeLoader import ShapeDataSet\n",
    "from torchvision import transforms as trans\n",
    "from torch.utils.data import Dataset, ConcatDataset, DataLoader, RandomSampler\n",
    "from torchvision import transforms as trans\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image, ImageFile\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "import torch\n",
    "import sys\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F \n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "%matplotlib inline\n",
    "from pathlib import Path\n",
    "from torch import nn\n",
    "from torchvision import transforms as trans\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "import os\n",
    "from itertools import product\n",
    "import re\n",
    "from tqdm.notebook import tqdm as tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 resnet18 models generated\n"
     ]
    }
   ],
   "source": [
    "conf = get_config(2)\n",
    "conf.device = 'cpu'\n",
    "conf.net_mode = 'resnet18'\n",
    "conf.n_shapes = 1\n",
    "conf.n_colors = 3\n",
    "conf.shape_only = False\n",
    "conf.color_only = False\n",
    "learner = ShapeLearner(conf, inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "triangle_ds = ShapeDataSet(no_bkg=True)\n",
    "triangle_ds.shapes = ['triangle']\n",
    "triangle_ds.colors = [[(255, 255), (0, 0), (0, 0)],\n",
    "                       [(0, 0), (255, 255), (0, 0)],\n",
    "                       [(0, 0), (0, 0), (255, 255)]]\n",
    "triangle_ds.n_shapes = 1\n",
    "triangle_ds.n_colors = 3\n",
    "ziped_classes = enumerate(product(range(1), range(3)))\n",
    "triangle_ds.label_map = {v:-1 for k, v in ziped_classes}\n",
    "triangle_ds.label_names = [-1]\n",
    "learner.ds = triangle_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate OOD shape/color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_distractors(learner):\n",
    "    # set OOD data\n",
    "    triangle_ds = ShapeDataSet(no_bkg=True)\n",
    "    triangle_ds.shapes = ['triangle']\n",
    "    triangle_ds.colors = [[(255, 255), (0, 0), (0, 0)],\n",
    "                           [(0, 0), (255, 255), (0, 0)],\n",
    "                           [(0, 0), (0, 0), (255, 255)]]\n",
    "    triangle_ds.n_shapes = 1\n",
    "    triangle_ds.n_colors = 3\n",
    "    ziped_classes = enumerate(product(range(1), range(3)))\n",
    "    triangle_ds.label_map = {v:-1 for k, v in ziped_classes}\n",
    "    triangle_ds.label_names = [-1]\n",
    "    learner.ds = triangle_ds\n",
    "    \n",
    "    dloader_args = {\n",
    "            'batch_size': conf.batch_size,\n",
    "            'pin_memory': True,\n",
    "            'num_workers': conf.num_workers,\n",
    "            'drop_last': False,\n",
    "    }\n",
    "    learner.loader = DataLoader(learner.ds, **dloader_args)\n",
    "    eval_sampler = RandomSampler(learner.ds, replacement=True, num_samples=len(learner.ds) // 10)\n",
    "    learner.eval_loader = DataLoader(learner.ds, sampler=eval_sampler, **dloader_args)\n",
    "\n",
    "def set_probes(learner):\n",
    "    # set OOD data\n",
    "    triangle_ds = ShapeDataSet(no_bkg=True)\n",
    "    triangle_ds.shapes = ['rectangle', 'circle']\n",
    "    triangle_ds.colors = [[(255, 255), (0, 0), (0, 0)],\n",
    "                           [(0, 0), (255, 255), (0, 0)],\n",
    "                           [(0, 0), (0, 0), (255, 255)]]\n",
    "    triangle_ds.n_shapes = 2\n",
    "    triangle_ds.n_colors = 3 # TODO fix this! we need the right 2 in the right order!\n",
    "    ziped_classes = enumerate(product(range(2), range(3)))\n",
    "    triangle_ds.label_map = {v:-1 for k, v in ziped_classes}\n",
    "    triangle_ds.label_names = [-1]\n",
    "    learner.ds = triangle_ds\n",
    "    \n",
    "    dloader_args = {\n",
    "            'batch_size': conf.batch_size,\n",
    "            'pin_memory': True,\n",
    "            'num_workers': conf.num_workers,\n",
    "            'drop_last': False,\n",
    "    }\n",
    "    learner.loader = DataLoader(learner.ds, **dloader_args)\n",
    "    eval_sampler = RandomSampler(learner.ds, replacement=True, num_samples=len(learner.ds) // 10)\n",
    "    learner.eval_loader = DataLoader(learner.ds, sampler=eval_sampler, **dloader_args)\n",
    "    \n",
    "def get_evaluation(learner):\n",
    "    # evaluate OOD data\n",
    "    for i in range(len(learner.models)):\n",
    "        learner.models[i].eval()\n",
    "    do_mean = -1 if len(learner.models) > 1 else 0\n",
    "    ind_iter = range(do_mean, len(learner.models))\n",
    "    predictions = dict(zip(ind_iter, [[] for i in ind_iter]))\n",
    "    prob = dict(zip(ind_iter, [[] for i in ind_iter]))\n",
    "    labels = []\n",
    "    learner.eval_loader.dataset.set_mode('test')  # todo check this works :)\n",
    "    for imgs, label in tqdm_notebook(learner.eval_loader, total=len(learner.eval_loader)):\n",
    "        imgs = imgs.to(conf.device)\n",
    "        thetas = [model(imgs).detach() for model in learner.models]\n",
    "        if len(learner.models) > 1: thetas = [torch.mean(torch.stack(thetas), 0)] + thetas\n",
    "        for ind, theta in zip(range(do_mean, len(learner.models)), thetas):\n",
    "            val, arg = torch.max(theta, dim=1)\n",
    "            predictions[ind].append(arg.cpu().numpy())\n",
    "            prob[ind].append(theta.cpu().numpy())\n",
    "        labels.append(label.detach().cpu().numpy())\n",
    "\n",
    "    labels = np.hstack(labels)\n",
    "    for ind in range(do_mean, len(leaner.models)):\n",
    "        predictions[ind] = np.hstack(predictions[ind])\n",
    "        prob[ind] = np.vstack(prob[ind])\n",
    "    return prob, predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "triangle_ds = ShapeDataSet(no_bkg=True)\n",
    "triangle_ds.shapes = ['triangle']\n",
    "triangle_ds.colors = [[(255, 255), (0, 0), (0, 0)],\n",
    "                       [(0, 0), (255, 255), (0, 0)],\n",
    "                       [(0, 0), (0, 0), (255, 255)]]\n",
    "triangle_ds.n_shapes = 1\n",
    "triangle_ds.n_colors = 3\n",
    "ziped_classes = enumerate(product(range(1), range(3)))\n",
    "triangle_ds.label_map = {v:-1 for k, v in ziped_classes}\n",
    "triangle_ds.label_names = [-1]\n",
    "learner.ds = triangle_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dloader_args = {\n",
    "            'batch_size': conf.batch_size,\n",
    "            'pin_memory': True,\n",
    "            'num_workers': conf.num_workers,\n",
    "            'drop_last': False,\n",
    "}\n",
    "learner.loader = DataLoader(learner.ds, **dloader_args)\n",
    "eval_sampler = RandomSampler(learner.ds, replacement=True, num_samples=len(learner.ds) // 10)\n",
    "learner.eval_loader = DataLoader(learner.ds, sampler=eval_sampler, **dloader_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = 'work_space/save/shapes_pre'\n",
    "rel_dirs = os.listdir(MODEL_DIR)\n",
    "alpha = [re.findall('a=([0-9, \\.]*)_', d) for d in rel_dirs]\n",
    "fix_str = [x for x in os.listdir(path.join(MODEL_DIR, rel_dirs[0])) if 'model' in x][0][8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = rel_dirs[0]\n",
    "conf.save_path = Path(path.join(MODEL_DIR, model_path))\n",
    "learner.load_state(conf, fix_str, model_only=True, from_save_folder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate OOD samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89e6ce371374b87a53fbfd1d3308a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'leaner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-94576a78fc61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# distractors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mset_distractors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdistractors_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistractors_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistractors_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# probs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-d4c0341fb9d3>\u001b[0m in \u001b[0;36mget_evaluation\u001b[0;34m(learner)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'leaner' is not defined"
     ]
    }
   ],
   "source": [
    "# distractors\n",
    "set_distractors(learner)\n",
    "distractors_prob, distractors_predictions, distractors_labels = get_evaluation(learner)\n",
    "\n",
    "# probs\n",
    "set_probes(learner)\n",
    "prob_prob, prob_predictions, prob_labels = get_evaluation(learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## estimate OOD scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scores(mat_path, matches_path):\n",
    "    n_queries = len(json.load(open(matches_path)))\n",
    "    mat = np.fromfile(mat_path, dtype=np.float32)[4:]\n",
    "    n_targets = len(mat) // n_queries\n",
    "    return mat.reshape(n_queries, n_targets)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(match_path, distractor_mat_path, prob_mat_path, features_path):\n",
    "    prob_features = json.load(open(features_path))\n",
    "    distractor_mat = load_scores(distractor_mat_path, match_path)\n",
    "    prob_mat = load_scores(prob_mat_path, match_path)\n",
    "    np.fill_diagonal(prob_mat, prob_mat.min() - 1) # self dist is not interesting!\n",
    "    prob_ids = np.array(prob_features['id'])\n",
    "    return prob_mat, distractor_mat, prob_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_fuse(mat_1, mat_2):\n",
    "    return (mat_1 + mat_2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_path = \"results/2020-02-15/matches_facescrub_megaface_2020-02-15_1000000_1.json\"\n",
    "distractor_mat_path = \"results/2020-02-15/otherFiles/facescrub_megaface_2020-02-15_1000000_1.bin\"\n",
    "prob_mat_path = \"results/2020-02-15/otherFiles/facescrub_facescrub_2020-02-15.bin\"\n",
    "features_path = \"results/2020-02-15/otherFiles/facescrub_features_2020-02-15\"\n",
    "\n",
    "prob_features = json.load(open(features_path))\n",
    "distractor_mat = load_scores(distractor_mat_path, match_path)\n",
    "prob_mat = load_scores(prob_mat_path, match_path)\n",
    "np.fill_diagonal(prob_mat, prob_mat.min() - 1) # self dist is not interesting!\n",
    "tot_mat = np.concatenate((prob_mat, distractor_mat), axis=1)\n",
    "prob_ids = np.array(prob_features['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gallary():\n",
    "    samples = pd.DataFrame([prob_ids, np.arange(len(prob_ids))]).T.groupby(0).apply(lambda x: x.sample(frac=.5).index[0]).sort_values()\n",
    "    return samples.values, samples.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank1_scores(prob_mat, distractor_mat, prob_ids):\n",
    "    \"\"\" \n",
    "        is the target id in the top 1 (not including self image)\n",
    "        (prob_mat assuming diag is nan)\n",
    "        prob_ids are a ground truth of ids ordered probmat entries\n",
    "    \"\"\"\n",
    "    \n",
    "    out_max = np.max(distractor_mat, axis=1) # best out of distractor set\n",
    "    np.fill_diagonal(prob_mat, prob_mat.min() - 1)\n",
    "    tot_mat = np.concatenate((prob_mat, out_max.reshape(3506, 1)), axis=1) # working with a small matrix :)\n",
    "    return np.max(tot_mat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank1_(prob_mat, distractor_mat, prob_ids):\n",
    "    \"\"\" \n",
    "        is the target id in the top 1 (not including self image)\n",
    "        (prob_mat assuming diag is nan)\n",
    "        prob_ids are a ground truth of ids ordered probmat entries\n",
    "    \"\"\"\n",
    "    \n",
    "    out_max = np.max(distractor_mat, axis=1) # best out of distractor set\n",
    "    tot_mat = np.concatenate((prob_mat, out_max.reshape(3506, 1)), axis=1) # working with a small matrix :)\n",
    "    n_probs = prob_mat.shape[1]\n",
    "    inf_ = tot_mat.min() - 1\n",
    "    score = 0\n",
    "    n_query = 0 \n",
    "    for query_id in set(prob_ids):\n",
    "        # probes from iden\n",
    "        row_mask = list((query_id == prob_ids))\n",
    "        curr_id_mat = tot_mat[row_mask, :]\n",
    "        \n",
    "        n_id_probs, n_probs_and_one_other = curr_id_mat.shape\n",
    "        query_indices = np.argwhere(row_mask).T[0]\n",
    "        for row_i, col_i in enumerate(query_indices):\n",
    "            # nullify all other cols from indices\n",
    "            col_mask = row_mask.copy()\n",
    "            col_mask[col_i] = False\n",
    "            col_mask.append(False)\n",
    "\n",
    "            other_probs_sub = curr_id_mat[: , col_mask].copy()\n",
    "            curr_id_mat[: , col_mask] = inf_\n",
    "\n",
    "            # calc hits\n",
    "            top_hits = np.delete(np.argmax(curr_id_mat, axis=1), row_i ,0)\n",
    "            top_id = np.where(top_hits < n_probs, top_hits, 0)\n",
    "            top_id = np.where(top_hits < n_probs, prob_ids[top_id], 'NaN')\n",
    "            score += sum(top_id == query_id)\n",
    "            n_query += len(top_id)\n",
    "\n",
    "            # get them back\n",
    "            curr_id_mat[:, col_mask] = other_probs_sub\n",
    "            \n",
    "    return score/n_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9674149740665954"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank1_(prob_mat, distractor_mat, prob_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank10_(prob_mat, distractor_mat, prob_ids):\n",
    "    \"\"\" \n",
    "        is the target id in the top 1 (not including self image)\n",
    "        (prob_mat assuming diag is nan)\n",
    "        prob_ids are a ground truth of ids ordered probmat entries\n",
    "    \"\"\"\n",
    "    \n",
    "    tot_mat = np.concatenate((prob_mat, distractor_mat), axis=1) # working with a small matrix :)\n",
    "    n_probs = prob_mat.shape[1]\n",
    "    inf_ = tot_mat.min() - 1\n",
    "    score = 0\n",
    "    n_query = 0 \n",
    "    for query_id in set(prob_ids):\n",
    "        # probes from iden\n",
    "        row_mask = list((query_id == prob_ids))\n",
    "        curr_id_mat = tot_mat[row_mask, :]\n",
    "        \n",
    "        n_id_probs, n_probs_and_one_other = curr_id_mat.shape\n",
    "        query_indices = np.argwhere(row_mask).T[0]\n",
    "        for row_i, col_i in enumerate(query_indices):\n",
    "            # nullify all other cols from indices\n",
    "            col_mask = row_mask.copy()\n",
    "            col_mask[col_i] = False\n",
    "            col_mask.extend([False]*distractor_mat.shape[1])\n",
    "\n",
    "            other_probs_sub = curr_id_mat[: , col_mask].copy()\n",
    "            curr_id_mat[: , col_mask] = inf_\n",
    "\n",
    "            # calc hits\n",
    "            top_hits = np.argpartition(curr_id_mat, -10, axis=1)[:, -10:]\n",
    "            # top hit might be wrong\n",
    "            top_correct = np.where(top_hits < len(prob_ids), top_hits, np.nan).astype(np.int)\n",
    "            # TODO from here\n",
    "            score += sum([np.isin(true_id, prob_ids[top_hit_row]) for top_hit_row,true_id in zip(top_correct, prob_ids)])\n",
    "            n_query += len(top_id)\n",
    "\n",
    "            # get them back\n",
    "            curr_id_mat[:, col_mask] = other_probs_sub\n",
    "            \n",
    "    return score/n_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank10(tot_mat, prob_ids):\n",
    "    \"\"\"\n",
    "        is the target id in the top 10 (not including self image)\n",
    "        prob_mat assuming diag is nan\n",
    "        prob_ids are a ground truth of ids ordered probmat entries\n",
    "    \"\"\"\n",
    "    \n",
    "    # works great from here do not touch :)\n",
    "    top_hits = np.argpartition(tot_mat, -10, axis=1)[:, -10:]\n",
    "    # top hit might be wrong\n",
    "    top_correct = np.where(top_hits < len(prob_ids), top_hits, np.nan).astype(np.int)\n",
    "    return sum([np.isin(true_id, prob_ids[top_hit_row]) for top_hit_row,true_id in zip(top_correct, prob_ids)]) / len(prob_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index -9223372036854775808 is out of bounds for axis 0 with size 3506",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-381-99821ea5f3fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrank10_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistractor_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-380-48b535b311b6>\u001b[0m in \u001b[0;36mrank10_\u001b[0;34m(prob_mat, distractor_mat, prob_ids)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# top hit might be wrong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtop_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_hits\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_hits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop_hit_row\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtop_hit_row\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrue_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mn_query\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-380-48b535b311b6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# top hit might be wrong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtop_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_hits\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_hits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop_hit_row\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtop_hit_row\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrue_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mn_query\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index -9223372036854775808 is out of bounds for axis 0 with size 3506"
     ]
    }
   ],
   "source": [
    "rank10_(prob_mat, distractor_mat, np.array(prob_features['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_set_label_and_score(tot_mat, prob_ids):\n",
    "    \"\"\"\n",
    "         What is the 2nd iden for the prob if he is not in set?\n",
    "         What is it's score?\n",
    "    \"\"\"\n",
    "    # works great from here do not touch :)\n",
    "    trail = tot_mat.shape[1]-len(prob_ids)\n",
    "    id_set = set(prob_ids)\n",
    "    res_label = []\n",
    "    res_score = []\n",
    "    inf_ = tot_mat.min() - 1\n",
    "    for curr_id in id_set:\n",
    "        id_mask = (prob_ids == curr_id)\n",
    "        curr_queries = tot_mat[id_mask]\n",
    "        curr_queries[:, np.concatenate((id_mask, np.zeros(trail).astype(bool)))] = inf_\n",
    "        \n",
    "        top_scores_for_id_queries = np.max(curr_queries, axis=1)\n",
    "        top_hits_for_id_queries = np.argmax(curr_queries, axis=1)\n",
    "        top_correct = np.where(top_hits_for_id_queries < len(prob_ids), top_hits_for_id_queries, 0)\n",
    "        top_2nd_probe_id_or_none = np.where(top_hits_for_id_queries < len(prob_ids), top_correct, np.nan).astype(np.int)\n",
    "        res_label.extend(top_2nd_probe_id_or_none)\n",
    "        res_score.extend(top_scores_for_id_queries)\n",
    "    return res_label, res_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_set_label_and_score_(prob_mat, distractor_mat, prob_ids):\n",
    "    \"\"\"\n",
    "         What is the 2nd iden for the prob if he is not in set?\n",
    "         What is it's score?\n",
    "    \"\"\"\n",
    "    open_best_score = np.max(distractor_mat, axis=1)\n",
    "    open_best_ind = np.argmax(distractor_mat, axis=1)\n",
    "    tot_mat = np.concatenate((prob_mat, out_max.reshape(3506, 1)), axis=1) # working with a small matrix :)\n",
    "    \n",
    "    id_set = set(prob_ids)\n",
    "    res_label = []\n",
    "    res_score = []\n",
    "    inf_ = tot_mat.min() - 1\n",
    "    for curr_id in id_set:\n",
    "        # probes from iden\n",
    "        row_mask = list((curr_id == prob_ids))\n",
    "        curr_id_mat = tot_mat[row_mask, :]\n",
    "        \n",
    "        n_id_probs, n_probs_and_one_other = curr_id_mat.shape\n",
    "        query_indices = np.argwhere(row_mask).T[0]\n",
    "\n",
    "        # nullify all query cols from indices\n",
    "        col_mask = row_mask.copy()\n",
    "        col_mask.append(False)\n",
    "\n",
    "        other_probs_sub = curr_id_mat[: , col_mask].copy()\n",
    "        curr_id_mat[: , col_mask] = inf_\n",
    "\n",
    "        # calc hits\n",
    "        top_hits = np.argmax(curr_id_mat, axis=1)\n",
    "        top_hits = np.argmax(curr_id_mat, axis=1)\n",
    "        top_id = np.where(top_hits < n_probs, top_hits, 0)\n",
    "        top_id = np.where(top_hits < n_probs, prob_ids[top_id], open_best_ind)\n",
    "        \n",
    "        \n",
    "        # get them back\n",
    "        curr_id_mat[:, col_mask] = other_probs_sub\n",
    "\n",
    "\n",
    "\n",
    "        id_mask = list((prob_ids == curr_id))\n",
    "        curr_queries = tot_mat[id_mask]\n",
    "        curr_queries[:, np.concatenate((id_mask, np.zeros(trail).astype(bool)))] = inf_\n",
    "        \n",
    "        top_scores_for_id_queries = np.max(curr_queries, axis=1)\n",
    "        top_hits_for_id_queries = np.argmax(curr_queries, axis=1)\n",
    "        top_correct = np.where(top_hits_for_id_queries < len(prob_ids), top_hits_for_id_queries, 0)\n",
    "        top_2nd_probe_id_or_none = np.where(top_hits_for_id_queries < len(prob_ids), top_correct, np.nan).astype(np.int)\n",
    "        res_label.extend(top_2nd_probe_id_or_none)\n",
    "        res_score.extend(top_scores_for_id_queries)\n",
    "    return res_label, res_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, scores = open_set_label_and_score_(tot_mat, np.array(prob_features['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO open set correlations between 2 models\n",
    "\"\"\"\n",
    "    for m1 in range(modelsNb):\n",
    "        for m2 in range(m1+1, modelsNb):\n",
    "            a = np.sum((open_set_1st_labels[m1] == open_set_1st_labels[m2]))\n",
    "            b = len(query_label[0])\n",
    "            print('Corr=', model_names[m1], model_names[m2], a, b, a/b)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_models(probe_mat_0, distractor_mat_0, probe_mat1, distractor_mat_1, prob_ids):\n",
    "    top1_scores_0 = top1_scores(probe_mat_0, distractor_mat_0, prob_ids)\n",
    "    top1_scores_1 = top1_scores(probe_mat_1, distractor_mat_1, prob_ids)\n",
    "    \n",
    "    open_set_1st_labels_0, open_set_1st_scores_0  = open_set_label_and_score(probe_mat_0, distractor_mat_0, prob_ids)\n",
    "    open_set_1st_labels_1, open_set_1st_scores_1 = open_set_label_and_score(probe_mat_1, distractor_mat_1, prob_ids)\n",
    "    \n",
    "    a = np.sum((open_set_1st_labels_0 == open_set_1st_labels_1))\n",
    "    b = len(prob_ids)\n",
    "    print('  Corr=%0.1f%%' % (100.0*a / b) )\n",
    "\n",
    "    for target_FTR in target_FTRS:\n",
    "        for TH in np.arange(0.3, 0.9, 0.00001):\n",
    "            FTR = np.sum((open_set_1st_labels[m] == open_set_1st_labels[m+1]) & (open_set_1st_scores[m] > TH) & (open_set_1st_scores[m+1] > TH)) / b\n",
    "            if math.isclose(FTR, target_FTR, abs_tol = 0.0005):\n",
    "                TTR = np.sum((top1_scores[m] > TH) & (top1_scores[m+1] > TH)) / b\n",
    "                print('   FTR=%0.1f%%, TTR=%0.1f%%' % (FTR * 100, TTR * 100))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
